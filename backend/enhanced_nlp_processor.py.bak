import numpy as np
import re
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from collections import Counter
import sqlite3
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# For modern transformer models
try:
    from sentence_transformers import SentenceTransformer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("sentence-transformers not available. Install with: pip install sentence-transformers")

logger = logging.getLogger(__name__)

class EnhancedResumeMatcherNLP:
    """
    Improved Resume Matcher with simplified components and modern NLP.
    Reduces complexity from 5 components to 3 core components.
    """
    
    def __init__(self, model_path: str = 'all-MiniLM-L6-v2'):
        self.job_embeddings = {}
        self.resume_embeddings = {}
        self.job_texts = {}
        self.resume_texts = {}
        self.all_texts = []
        self.corpus_fitted = False
        
        # Feedback storage for continuous learning
        self.feedback_db = "feedback.db"
        self._init_feedback_db()
        
        # Initialize NLTK components
        self._init_nltk()
        
        # Initialize TF-IDF vectorizer (simplified)
        self._init_tfidf()
        
        # Initialize transformer model for semantic understanding
        self._init_transformer(model_path)
        
        # Skill categories for weighted matching
        self.skill_weights = {
            'programming': 3.0,
            'frameworks': 2.5,
            'cloud': 2.5,
            'databases': 2.0,
            'tools': 1.5,
            'soft_skills': 1.0
        }
        
        logger.info("Enhanced NLP processor initialized successfully")

    def _init_nltk(self):
        """Initialize NLTK components"""
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('wordnet', quiet=True)
            
            self.stop_words = set(stopwords.words('english'))
            self.lemmatizer = WordNetLemmatizer()
        except Exception as e:
            logger.error(f"Error initializing NLTK: {str(e)}")
            raise

    def _init_tfidf(self):
        """Initialize simplified TF-IDF vectorizer"""
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=3000,  # Reduced from 5000 to avoid noise
            stop_words='english',
            ngram_range=(1, 2),  # Simplified to unigrams and bigrams only
            lowercase=True,
            min_df=1,
            max_df=0.85,
            sublinear_tf=True,
            norm='l2'
        )

    def _init_transformer(self, model_path: str):
        """Initialize transformer model for semantic understanding"""
        self.transformer_model = None
        if TRANSFORMERS_AVAILABLE:
            try:
                self.transformer_model = SentenceTransformer(model_path)
                logger.info(f"Loaded transformer model: {model_path}")
            except Exception as e:
                logger.warning(f"Could not load transformer model: {str(e)}")
                logger.warning("Falling back to TF-IDF only semantic analysis")

    def _init_feedback_db(self):
        """Initialize SQLite database for storing feedback"""
        try:
            conn = sqlite3.connect(self.feedback_db)
            cursor = conn.cursor()
            
            # Create feedback table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS feedback (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    job_id TEXT NOT NULL,
                    resume_id TEXT NOT NULL,
                    predicted_score REAL NOT NULL,
                    human_score REAL,
                    recruiter_feedback TEXT,
                    match_quality TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Create validation metrics table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS validation_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    metric_name TEXT NOT NULL,
                    value REAL NOT NULL,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            logger.info("Feedback database initialized")
        except Exception as e:
            logger.error(f"Error initializing feedback database: {str(e)}")

    def calculate_similarity(self, job_id: str, resume_id: str) -> float:
        """
        Simplified 3-component similarity calculation:
        1. Semantic Similarity (50%) - BERT embeddings or TF-IDF fallback
        2. Skill Matching (35%) - Weighted skill extraction and comparison
        3. Content Similarity (15%) - Traditional TF-IDF content matching
        """
        try:
            if job_id not in self.job_embeddings or resume_id not in self.resume_embeddings:
                logger.warning(f"Missing embeddings for job {job_id} or resume {resume_id}")
                return 0.0, 0.0  # score, confidence
            
            # Component 1: Semantic Similarity (50%)
            semantic_score = self._calculate_semantic_similarity(job_id, resume_id)
            
            # Component 2: Skill Matching (35%)
            skill_score = self._calculate_skill_similarity(job_id, resume_id)
            
            # Component 3: Content Similarity (15%)
            content_score = self._calculate_content_similarity(job_id, resume_id)
            
            # Calculate weighted final score
            final_score = (
                0.50 * semantic_score +
                0.35 * skill_score +
                0.15 * content_score
            )
            
            # Calculate confidence score
            confidence = self._calculate_confidence(semantic_score, skill_score, content_score, job_id, resume_id)
            
            # Store prediction for validation
            self._store_prediction(job_id, resume_id, final_score)
            
            return max(0.0, min(1.0, final_score)), confidence
            
        except Exception as e:
            logger.error(f"Error calculating similarity: {str(e)}")
            return 0.0, 0.0

    def _calculate_semantic_similarity(self, job_id: str, resume_id: str) -> float:
        """Calculate semantic similarity using transformer model or TF-IDF fallback"""
        try:
            job_text = self.job_texts.get(job_id, '')
            resume_text = self.resume_texts.get(resume_id, '')
            
            if not job_text or not resume_text:
                return 0.0
            
            # Use transformer model if available
            if self.transformer_model:
                try:
                    job_embedding = self.transformer_model.encode([job_text])
                    resume_embedding = self.transformer_model.encode([resume_text])
                    similarity = cosine_similarity(job_embedding, resume_embedding)[0][0]
                    return max(0.0, min(1.0, similarity))
                except Exception as e:
                    logger.warning(f"Transformer similarity failed: {e}, using TF-IDF fallback")
            
            # Fallback to TF-IDF
            return self._tfidf_similarity(job_text, resume_text)
            
        except Exception as e:
            logger.error(f"Error calculating semantic similarity: {str(e)}")
            return 0.0

    def _calculate_skill_similarity(self, job_id: str, resume_id: str) -> float:
        """Calculate weighted skill matching similarity"""
        try:
            job_skills = self.extract_skills(self.job_texts.get(job_id, ''))
            resume_skills = self.extract_skills(self.resume_texts.get(resume_id, ''))
            
            if not job_skills:
                return 0.5  # Neutral if no job skills found
            
            # Calculate weighted Jaccard similarity
            job_skill_weights = sum(self._get_skill_weight(skill) for skill in job_skills)
            matched_weights = sum(self._get_skill_weight(skill) for skill in job_skills if skill in resume_skills)
            
            if job_skill_weights == 0:
                return 0.5
            
            weighted_similarity = matched_weights / job_skill_weights
            
            # Boost score if candidate has additional relevant skills
            bonus_skills = set(resume_skills) - set(job_skills)
            bonus_score = min(0.2, len(bonus_skills) * 0.02)  # Max 20% bonus
            
            return min(1.0, weighted_similarity + bonus_score)
            
        except Exception as e:
            logger.error(f"Error calculating skill similarity: {str(e)}")
            return 0.0

    def _calculate_content_similarity(self, job_id: str, resume_id: str) -> float:
        """Calculate TF-IDF content similarity"""
        try:
            job_text = self.job_embeddings.get(job_id, '')
            resume_text = self.resume_embeddings.get(resume_id, '')
            
            if not job_text or not resume_text:
                return 0.0
            
            return self._tfidf_similarity(job_text, resume_text)
            
        except Exception as e:
            logger.error(f"Error calculating content similarity: {str(e)}")
            return 0.0

    def _calculate_confidence(self, semantic_score: float, skill_score: float, 
                            content_score: float, job_id: str, resume_id: str) -> float:
        """
        Calculate confidence score based on:
        - Score consistency across components
        - Data quality indicators
        - Historical accuracy for similar cases
        """
        try:
            # Component consistency (higher variance = lower confidence)
            scores = [semantic_score, skill_score, content_score]
            score_variance = np.var(scores)
            consistency_score = max(0.0, 1.0 - (score_variance * 2))  # Penalize high variance
            
            # Data quality factors
            job_text = self.job_texts.get(job_id, '')
            resume_text = self.resume_texts.get(resume_id, '')
            
            # Text length indicators (too short = low confidence)
            job_length_factor = min(1.0, len(job_text.split()) / 50)  # Ideal ~50+ words
            resume_length_factor = min(1.0, len(resume_text.split()) / 100)  # Ideal ~100+ words
            
            # Skill detection quality (more skills = higher confidence)
            job_skills = self.extract_skills(job_text)
            resume_skills = self.extract_skills(resume_text)
            skill_factor = min(1.0, (len(job_skills) + len(resume_skills)) / 20)  # Ideal ~20+ skills total
            
            # Overall confidence
            confidence = (
                0.4 * consistency_score +
                0.25 * job_length_factor +
                0.25 * resume_length_factor +
                0.1 * skill_factor
            )
            
            return max(0.1, min(1.0, confidence))  # Ensure confidence is between 10-100%
            
        except Exception as e:
            logger.error(f"Error calculating confidence: {str(e)}")
            return 0.5

    def extract_skills(self, text: str) -> List[str]:
        """Extract and categorize skills from text"""
        skills_db = {
            'programming': [
                'python', 'javascript', 'java', 'typescript', 'c++', 'c#', 'php', 'ruby',
                'go', 'rust', 'swift', 'kotlin', 'scala', 'r', 'matlab', 'sql'
            ],
            'frameworks': [
                'react', 'angular', 'vue', 'django', 'flask', 'spring', 'laravel',
                'rails', 'express', 'nextjs', 'nodejs', 'asp.net', 'bootstrap', 'tailwind'
            ],
            'cloud': [
                'aws', 'azure', 'gcp', 'google cloud', 'docker', 'kubernetes',
                'jenkins', 'terraform', 'ansible', 'microservices', 'serverless'
            ],
            'databases': [
                'postgresql', 'mysql', 'mongodb', 'redis', 'elasticsearch',
                'cassandra', 'dynamodb', 'sqlite', 'oracle', 'nosql'
            ],
            'tools': [
                'git', 'github', 'jira', 'confluence', 'slack', 'vscode',
                'intellij', 'postman', 'swagger', 'junit', 'pytest', 'selenium'
            ],
            'soft_skills': [
                'leadership', 'communication', 'problem solving', 'teamwork',
                'project management', 'agile', 'scrum', 'analytical thinking'
            ]
        }
        
        found_skills = []
        text_lower = text.lower()
        
        for category, skills in skills_db.items():
            for skill in skills:
                if re.search(r'\b' + re.escape(skill) + r'\b', text_lower):
                    found_skills.append(skill)
        
        return list(set(found_skills))

    def _get_skill_weight(self, skill: str) -> float:
        """Get weight for a skill based on its category"""
        skills_db = {
            'programming': ['python', 'javascript', 'java', 'typescript', 'c++', 'c#', 'php', 'ruby'],
            'frameworks': ['react', 'angular', 'vue', 'django', 'flask', 'spring', 'laravel'],
            'cloud': ['aws', 'azure', 'gcp', 'docker', 'kubernetes', 'jenkins'],
            'databases': ['postgresql', 'mysql', 'mongodb', 'redis', 'elasticsearch'],
            'tools': ['git', 'github', 'jira', 'junit', 'pytest', 'selenium'],
            'soft_skills': ['leadership', 'communication', 'problem solving', 'teamwork']
        }
        
        for category, skills in skills_db.items():
            if skill in skills:
                return self.skill_weights.get(category, 1.0)
        
        return 1.0  # Default weight

    def _tfidf_similarity(self, text1: str, text2: str) -> float:
        """Calculate TF-IDF cosine similarity between two texts"""
        try:
            if not text1 or not text2:
                return 0.0
            
            texts = [text1, text2]
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            return max(0.0, min(1.0, similarity))
        except Exception as e:
            logger.warning(f"TF-IDF similarity calculation failed: {e}")
            return 0.0

    def add_feedback(self, job_id: str, resume_id: str, human_score: float, 
                    recruiter_feedback: str, match_quality: str):
        """Add human feedback for continuous learning"""
        try:
            conn = sqlite3.connect(self.feedback_db)
            cursor = conn.cursor()
            
            # Get the predicted score
            cursor.execute('''
                SELECT predicted_score FROM feedback 
                WHERE job_id = ? AND resume_id = ? 
                ORDER BY timestamp DESC LIMIT 1
            ''', (job_id, resume_id))
            
            result = cursor.fetchone()
            predicted_score = result[0] if result else 0.0
            
            # Insert feedback
            cursor.execute('''
                UPDATE feedback 
                SET human_score = ?, recruiter_feedback = ?, match_quality = ?
                WHERE job_id = ? AND resume_id = ? AND human_score IS NULL
            ''', (human_score, recruiter_feedback, match_quality, job_id, resume_id))
            
            conn.commit()
            conn.close()
            
            logger.info(f"Feedback added for job {job_id}, resume {resume_id}")
            
        except Exception as e:
            logger.error(f"Error adding feedback: {str(e)}")

    def _store_prediction(self, job_id: str, resume_id: str, predicted_score: float):
        """Store prediction for later validation"""
        try:
            conn = sqlite3.connect(self.feedback_db)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO feedback (job_id, resume_id, predicted_score)
                VALUES (?, ?, ?)
            ''', (job_id, resume_id, predicted_score))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Error storing prediction: {str(e)}")

    def get_validation_metrics(self) -> Dict[str, float]:
        """Calculate validation metrics from feedback data"""
        try:
            conn = sqlite3.connect(self.feedback_db)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT predicted_score, human_score 
                FROM feedback 
                WHERE human_score IS NOT NULL
            ''')
            
            data = cursor.fetchall()
            conn.close()
            
            if len(data) < 5:  # Need at least 5 feedback entries
                return {"error": "Insufficient feedback data for validation"}
            
            predicted_scores = [row[0] for row in data]
            human_scores = [row[1] for row in data]
            
            # Calculate metrics
            mae = np.mean(np.abs(np.array(predicted_scores) - np.array(human_scores)))
            rmse = np.sqrt(np.mean((np.array(predicted_scores) - np.array(human_scores)) ** 2))
            correlation = np.corrcoef(predicted_scores, human_scores)[0, 1]
            
            # Accuracy within 20% threshold
            accuracy_20 = np.mean(np.abs(np.array(predicted_scores) - np.array(human_scores)) <= 0.2)
            
            return {
                "mae": float(mae),
                "rmse": float(rmse),
                "correlation": float(correlation),
                "accuracy_20_percent": float(accuracy_20),
                "sample_size": len(data)
            }
            
        except Exception as e:
            logger.error(f"Error calculating validation metrics: {str(e)}")
            return {"error": str(e)}

    def get_match_details(self, job_id: str, resume_id: str) -> Dict:
        """Get detailed match analysis including confidence and recommendations"""
        try:
            similarity, confidence = self.calculate_similarity(job_id, resume_id)
            
            job_skills = self.extract_skills(self.job_texts.get(job_id, ''))
            resume_skills = self.extract_skills(self.resume_texts.get(resume_id, ''))
            
            matched_skills = list(set(job_skills) & set(resume_skills))
            missing_skills = list(set(job_skills) - set(resume_skills))
            
            # Generate recommendations based on match quality and confidence
            recommendations = self._generate_recommendations(similarity, confidence, missing_skills)
            
            return {
                "similarity_score": similarity,
                "confidence_score": confidence,
                "match_strength": self._get_match_strength(similarity, confidence),
                "matched_skills": matched_skills,
                "missing_skills": missing_skills,
                "recommendations": recommendations,
                "metadata": {
                    "job_skills_count": len(job_skills),
                    "resume_skills_count": len(resume_skills),
                    "skill_match_ratio": len(matched_skills) / max(1, len(job_skills))
                }
            }
            
        except Exception as e:
            logger.error(f"Error getting match details: {str(e)}")
            return {"error": str(e)}

    def _get_match_strength(self, similarity: float, confidence: float) -> str:
        """Determine match strength based on similarity and confidence"""
        if similarity >= 0.8 and confidence >= 0.7:
            return "excellent"
        elif similarity >= 0.6 and confidence >= 0.6:
            return "good"
        elif similarity >= 0.4 and confidence >= 0.5:
            return "fair"
        elif similarity >= 0.2:
            return "poor"
        else:
            return "very_poor"

    def _generate_recommendations(self, similarity: float, confidence: float, 
                                missing_skills: List[str]) -> List[Dict]:
        """Generate actionable recommendations"""
        recommendations = []
        
        if confidence < 0.5:
            recommendations.append({
                "priority": "high",
                "message": "Low confidence in match - review requires human judgment",
                "type": "confidence"
            })
        
        if similarity < 0.3:
            recommendations.append({
                "priority": "high",
                "message": "Poor overall match - consider screening out",
                "type": "match_quality"
            })
        
        if len(missing_skills) > 5:
            recommendations.append({
                "priority": "medium",
                "message": f"Candidate missing {len(missing_skills)} key skills",
                "type": "skills_gap"
            })
        
        if missing_skills:
            high_priority_missing = [skill for skill in missing_skills[:3]]
            recommendations.append({
                "priority": "medium",
                "message": f"Focus interview on: {', '.join(high_priority_missing)}",
                "type": "interview_focus"
            })
        
        if similarity >= 0.7:
            recommendations.append({
                "priority": "low",
                "message": "Strong candidate - prioritize for interview",
                "type": "positive"
            })
        
        return recommendations

    # Inherit other necessary methods from the original class
    def preprocess_text(self, text: str) -> str:
        """Simplified text preprocessing"""
        try:
            text = text.lower()
            text = re.sub(r'[^\w\s\+\#\.\-]', ' ', text)
            text = re.sub(r'\s+', ' ', text).strip()
            
            tokens = word_tokenize(text)
            processed_tokens = []
            
            for token in tokens:
                if len(token) >= 2 and (token not in self.stop_words or 
                                       any(char in token for char in ['+', '#', '.'])):
                    if token.isalpha() and len(token) > 2:
                        processed_tokens.append(self.lemmatizer.lemmatize(token))
                    else:
                        processed_tokens.append(token)
            
            return ' '.join(processed_tokens)
            
        except Exception as e:
            logger.error(f"Error preprocessing text: {str(e)}")
            return text

    def process_job_description(self, job_id: str, job_text: str):
        """Process and store job description"""
        try:
            self.job_texts[job_id] = job_text
            processed_text = self.preprocess_text(job_text)
            self.job_embeddings[job_id] = processed_text
            
            if processed_text not in self.all_texts:
                self.all_texts.append(processed_text)
                self.corpus_fitted = False
            
            logger.info(f"Job description {job_id} processed successfully")
            
        except Exception as e:
            logger.error(f"Error processing job description {job_id}: {str(e)}")
            raise

    def process_resume(self, resume_id: str, resume_text: str):
        """Process and store resume"""
        try:
            self.resume_texts[resume_id] = resume_text
            processed_text = self.preprocess_text(resume_text)
            self.resume_embeddings[resume_id] = processed_text
            
            if processed_text not in self.all_texts:
                self.all_texts.append(processed_text)
                self.corpus_fitted = False
            
            logger.info(f"Resume {resume_id} processed successfully")
            
        except Exception as e:
            logger.error(f"Error processing resume {resume_id}: {str(e)}")
            raise

    def fit_corpus_vectorizers(self):
        """Fit corpus vectorizers (compatibility method for original system)"""
        try:
            if len(self.all_texts) >= 2:
                # This enhanced system doesn't need explicit corpus fitting
                # as it uses BERT embeddings, but we keep this for compatibility
                self.corpus_fitted = True
                logger.info(f"Corpus vectorizers fitted with {len(self.all_texts)} documents")
            else:
                logger.warning("Not enough texts to fit corpus vectorizers")
        except Exception as e:
            logger.error(f"Error fitting corpus vectorizers: {str(e)}")